{"timestamp":"2025-08-31T21:41:40.184498","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-08-31T21:41:40.186273","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/reddit_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-08-31T21:41:40.503326Z","level":"info","event":"Connected to reddit!","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.749032Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone,\\n\\nQuick question for the data professionals who\\'ve moved into more business-facing roles - how did you handle the communication transition?\\n\\nI\\'m a data scientist/engineer who recently got promoted, and I\\'m getting feedback that I\\'m \"too much into technical details\" and need to adapt my communication style for different stakeholders. The challenge is that my analytical, direct approach is what made me good at the technical work, but it\\'s not translating well to the business side.\\n\\nI\\'ve tried some of the usual suspects (Toastmasters, generic communication courses) but they all feel like they\\'re designed for sales people or public speakers, not engineers. The advice is either shallow (e.g. pace, filler words) or in theory (e.g. DISC framework) which doesn\\'t really help when your brain is wired to solve problems efficiently.\\n\\n**For those who\\'ve successfully made this transition - what actually moved the needle for you?**\\xa0Looking for practical advice, not just \"practice more.\"\\n\\nAlso, I\\'m working on something specifically for technical professionals facing this challenge. If you\\'ve been through this struggle, would you mind sharing your experience in a quick 8-question assessment? I want to build something that actually helps rather than adds to the pile of generic solutions.\\n\\n[https://docs.google.com/forms/d/e/1FAIpQLSfIPaUjV0Okcblh4MVkxF0kPgFww2EVQdYG7\\\\_cUfxQxR-Z8WA/viewform?usp=dialog](https://docs.google.com/forms/d/e/1FAIpQLSfIPaUjV0Okcblh4MVkxF0kPgFww2EVQdYG7_cUfxQxR-Z8WA/viewform?usp=dialog)\\n\\nGenuinely trying to learn from the community here - what worked, what didn\\'t, and what\\'s still missing?', 'author_fullname': 't2_f5n6unwxe', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data professionals who moved to business-facing roles - how did you handle the communication shift', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4n346', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 23, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756619960.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p>\\n\\n<p>Quick question for the data professionals who&#39;ve moved into more business-facing roles - how did you handle the communication transition?</p>\\n\\n<p>I&#39;m a data scientist/engineer who recently got promoted, and I&#39;m getting feedback that I&#39;m &quot;too much into technical details&quot; and need to adapt my communication style for different stakeholders. The challenge is that my analytical, direct approach is what made me good at the technical work, but it&#39;s not translating well to the business side.</p>\\n\\n<p>I&#39;ve tried some of the usual suspects (Toastmasters, generic communication courses) but they all feel like they&#39;re designed for sales people or public speakers, not engineers. The advice is either shallow (e.g. pace, filler words) or in theory (e.g. DISC framework) which doesn&#39;t really help when your brain is wired to solve problems efficiently.</p>\\n\\n<p><strong>For those who&#39;ve successfully made this transition - what actually moved the needle for you?</strong>\\xa0Looking for practical advice, not just &quot;practice more.&quot;</p>\\n\\n<p>Also, I&#39;m working on something specifically for technical professionals facing this challenge. If you&#39;ve been through this struggle, would you mind sharing your experience in a quick 8-question assessment? I want to build something that actually helps rather than adds to the pile of generic solutions.</p>\\n\\n<p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfIPaUjV0Okcblh4MVkxF0kPgFww2EVQdYG7_cUfxQxR-Z8WA/viewform?usp=dialog\">https://docs.google.com/forms/d/e/1FAIpQLSfIPaUjV0Okcblh4MVkxF0kPgFww2EVQdYG7_cUfxQxR-Z8WA/viewform?usp=dialog</a></p>\\n\\n<p>Genuinely trying to learn from the community here - what worked, what didn&#39;t, and what&#39;s still missing?</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?auto=webp&s=9b896a4ae9f50ba32e2f93eafd148d496e142dec', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?width=108&crop=smart&auto=webp&s=6a055f34bbb04f72ad38f713591865b1c32ecf85', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?width=216&crop=smart&auto=webp&s=b72f05f4a501a1c8de6b8fceff5af48b13c8c0fb', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?width=320&crop=smart&auto=webp&s=ef2fccbd598370c66f41ff1089b345dd7cbf4adf', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?width=640&crop=smart&auto=webp&s=5b9f326bdc762c612f2a099f8d55754db6a3ba48', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?width=960&crop=smart&auto=webp&s=a545d40c0a39af1e693030176e5593bab5fa9bba', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc.png?width=1080&crop=smart&auto=webp&s=9364fde5c0256fd16d730fe6e8dc107ebea2c4b5', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'gWmvjv4fSrJ1rj7IOVKuEwY98wxfsNgs7HLxz1ahopc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1n4n346', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BedroomSubstantial72'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4n346/data_professionals_who_moved_to_businessfacing/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4n346/data_professionals_who_moved_to_businessfacing/', 'subreddit_subscribers': 392322, 'created_utc': 1756619960.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.752439","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-08-31T21:41:41.784994Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Curious if anyone here has dealt with this situation:\\n\\nOur current data landscape is pretty scattered. There’s a push from the SAP side to make SAP Datasphere the central hub for all enterprise data, but in practice our data engineering team does almost everything in Databricks (pipelines, transformations, ML, analytics enablement, etc.).\\n\\nHas anyone faced the same tension between keeping data in SAP’s ecosystem vs consolidating in Databricks? How did you decide what belongs where, and how did you manage integration/governance without doubling effort?\\n\\nWould love to hear how others approached this.', 'author_fullname': 't2_l0i3wiu4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone else juggling SAP Datasphere vs Databricks as the “data hub”?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4rbsc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756636206.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Curious if anyone here has dealt with this situation:</p>\\n\\n<p>Our current data landscape is pretty scattered. There’s a push from the SAP side to make SAP Datasphere the central hub for all enterprise data, but in practice our data engineering team does almost everything in Databricks (pipelines, transformations, ML, analytics enablement, etc.).</p>\\n\\n<p>Has anyone faced the same tension between keeping data in SAP’s ecosystem vs consolidating in Databricks? How did you decide what belongs where, and how did you manage integration/governance without doubling effort?</p>\\n\\n<p>Would love to hear how others approached this.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1n4rbsc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='coldasicesup'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4rbsc/anyone_else_juggling_sap_datasphere_vs_databricks/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4rbsc/anyone_else_juggling_sap_datasphere_vs_databricks/', 'subreddit_subscribers': 392322, 'created_utc': 1756636206.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.786486Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone,\\n\\nI’m working on a data ingestion process in Azure and would like some guidance on the best strategy to extract data from an external API and store it directly in Azure Blob Storage (raw layer).\\n\\nThe idea is to have a simple flow that:\\n\\t1.\\tConsumes the API data (returned in JSON);\\n\\t2.\\tStores the files in a Blob container, so they can later be processed into the next layers (bronze, silver, gold).\\n\\nI’m evaluating a few options for this ingestion, such as:\\n\\t•\\tAzure Data Factory (using Copy Activity or Web Activity);\\n\\t•\\tAzure Functions to perform the extraction in a more serverless and scalable way.\\n\\nHas anyone here had practical experience with this type of scenario? What factors would you consider when choosing the tool, especially regarding costs, limitations, and performance?\\n\\nI’d also appreciate any tips on partitioning and naming standards for files in the raw layer, to avoid issues with maintenance and pipeline evolution in the future.', 'author_fullname': 't2_17dl35224h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to extract data from an API into Azure Blob (raw layer)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4k7z2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 14, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 14, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756610108.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p>\\n\\n<p>I’m working on a data ingestion process in Azure and would like some guidance on the best strategy to extract data from an external API and store it directly in Azure Blob Storage (raw layer).</p>\\n\\n<p>The idea is to have a simple flow that:\\n    1.  Consumes the API data (returned in JSON);\\n    2.  Stores the files in a Blob container, so they can later be processed into the next layers (bronze, silver, gold).</p>\\n\\n<p>I’m evaluating a few options for this ingestion, such as:\\n    • Azure Data Factory (using Copy Activity or Web Activity);\\n    • Azure Functions to perform the extraction in a more serverless and scalable way.</p>\\n\\n<p>Has anyone here had practical experience with this type of scenario? What factors would you consider when choosing the tool, especially regarding costs, limitations, and performance?</p>\\n\\n<p>I’d also appreciate any tips on partitioning and naming standards for files in the raw layer, to avoid issues with maintenance and pipeline evolution in the future.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1n4k7z2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No_Beautiful3867'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4k7z2/best_way_to_extract_data_from_an_api_into_azure/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4k7z2/best_way_to_extract_data_from_an_api_into_azure/', 'subreddit_subscribers': 392322, 'created_utc': 1756610108.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.787598Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi \\nI don’t have a proper career (worked in nannying, kindergarten teacher, hospitality etc and currently in marketing as a SM everything in a small company. )\\n\\nI have an educational background of Early Years Education and a recent MBA. \\n\\nMy background obviously is all over the place and I’m 29 which scares me even more.\\n\\nI currently came back to my home country with the plan to spend 12ish months locked in building skills to start a solid career (while working remotely for the company I’m in). \\n\\nAm I setting myself up for failure? \\n\\nI’m in between DA & DE , though DE appeals more to me. \\n\\nI also purchased a coursera plus membership in order to get access to learning resources. \\n\\nI want a reality check from you and all the advice you are willing to share. \\n\\nThank you 🙏 \\n\\n\\n\\n', 'author_fullname': 't2_a4nkw3hp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How long to become a DE?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4rhac', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.69, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756636794.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hi \\nI don’t have a proper career (worked in nannying, kindergarten teacher, hospitality etc and currently in marketing as a SM everything in a small company. )</p>\\n\\n<p>I have an educational background of Early Years Education and a recent MBA. </p>\\n\\n<p>My background obviously is all over the place and I’m 29 which scares me even more.</p>\\n\\n<p>I currently came back to my home country with the plan to spend 12ish months locked in building skills to start a solid career (while working remotely for the company I’m in). </p>\\n\\n<p>Am I setting myself up for failure? </p>\\n\\n<p>I’m in between DA &amp; DE , though DE appeals more to me. </p>\\n\\n<p>I also purchased a coursera plus membership in order to get access to learning resources. </p>\\n\\n<p>I want a reality check from you and all the advice you are willing to share. </p>\\n\\n<p>Thank you 🙏 </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1n4rhac', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Cautious_Canary8786'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4rhac/how_long_to_become_a_de/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4rhac/how_long_to_become_a_de/', 'subreddit_subscribers': 392322, 'created_utc': 1756636794.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.788708Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Hello everyone, I'm a data engineer in my day job with close to 2 decades of experience. I have been dabbling around in web development during my very limited free time for past several months. I have finally built my first real project - PulseHook, after working on it for last 2 months. I believe this tool/webapp can be useful for data engineering devs and teams. I am looking for the communities feedback. To be honest, I have never shared any of my work publicly and I'm a bit nervous.\\n\\nSo, the way PulseHook works is I have setup an api end point you can use to post from any of your scripts/jobs. You can send success and error status to this API endpoint. Also, you can setup the monitoring on the web app and enter email(s) and/or slack web hooks for notifications. If the api receives a failure status or job doesn't run in the intended duration, notification would be send to email(s) and/or slack. \\n\\nSo, here is the webapp link -\\xa0[https://www.pulsehook.app/](https://www.pulsehook.app/)\\xa0. Currently, I have not setup any monetization and its free to use. I would be really grateful for any feedback (good or bad :)).\\n\\nhttps://preview.redd.it/n6sbkqpoq9mf1.png?width=1575&format=png&auto=webp&s=0d1a7208199d77d6315fa0817f4fa12e1213aac3\\n\\n\", 'author_fullname': 't2_x63zu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': \"I'm a solo developer and just finished my first project. Its called PulseHook, a simple monitor for cron jobs. Looking for honest feedback!\", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 102, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'n6sbkqpoq9mf1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 79, 'x': 108, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=108&crop=smart&auto=webp&s=a813152c8a201b02423361271f984fa58814ad60'}, {'y': 158, 'x': 216, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=216&crop=smart&auto=webp&s=d8cc24d74feabd7f8b0f68503cb2a11aeaceb748'}, {'y': 234, 'x': 320, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=320&crop=smart&auto=webp&s=a7dc9d74db9592c438d8b611b95ae76dc251456d'}, {'y': 469, 'x': 640, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=640&crop=smart&auto=webp&s=100f6577b76b07785525872a52800bbaa9c7a578'}, {'y': 704, 'x': 960, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=960&crop=smart&auto=webp&s=d5bc1632b9eb17962b4078c710c560815e2763de'}, {'y': 792, 'x': 1080, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=1080&crop=smart&auto=webp&s=ed9bce1851e8ccd4d4e83af7517385aa36bf3d37'}], 's': {'y': 1155, 'x': 1575, 'u': 'https://preview.redd.it/n6sbkqpoq9mf1.png?width=1575&format=png&auto=webp&s=0d1a7208199d77d6315fa0817f4fa12e1213aac3'}, 'id': 'n6sbkqpoq9mf1'}}, 'name': 't3_1n4jrie', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/_jVX1pw8tCY6TPY_D6kEGPCXcmLkGvMVd3qvz-FapLI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756608647.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hello everyone, I&#39;m a data engineer in my day job with close to 2 decades of experience. I have been dabbling around in web development during my very limited free time for past several months. I have finally built my first real project - PulseHook, after working on it for last 2 months. I believe this tool/webapp can be useful for data engineering devs and teams. I am looking for the communities feedback. To be honest, I have never shared any of my work publicly and I&#39;m a bit nervous.</p>\\n\\n<p>So, the way PulseHook works is I have setup an api end point you can use to post from any of your scripts/jobs. You can send success and error status to this API endpoint. Also, you can setup the monitoring on the web app and enter email(s) and/or slack web hooks for notifications. If the api receives a failure status or job doesn&#39;t run in the intended duration, notification would be send to email(s) and/or slack. </p>\\n\\n<p>So, here is the webapp link -\\xa0<a href=\"https://www.pulsehook.app/\">https://www.pulsehook.app/</a>\\xa0. Currently, I have not setup any monetization and its free to use. I would be really grateful for any feedback (good or bad :)).</p>\\n\\n<p><a href=\"https://preview.redd.it/n6sbkqpoq9mf1.png?width=1575&amp;format=png&amp;auto=webp&amp;s=0d1a7208199d77d6315fa0817f4fa12e1213aac3\">https://preview.redd.it/n6sbkqpoq9mf1.png?width=1575&amp;format=png&amp;auto=webp&amp;s=0d1a7208199d77d6315fa0817f4fa12e1213aac3</a></p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1n4jrie', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ChunkyMonke'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4jrie/im_a_solo_developer_and_just_finished_my_first/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4jrie/im_a_solo_developer_and_just_finished_my_first/', 'subreddit_subscribers': 392322, 'created_utc': 1756608647.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.789659Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"I am looking for good schema evolution support and not a complex setup. \\n\\nWhat are you thoughts on using Snowflake's Openflow vs debezium vs AWS DMS vs SAAS solution\\n\\nWhat do you guys use? \", 'author_fullname': 't2_649rb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Postgres to Snowflake replication recommendations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4dy5z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756591523.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>I am looking for good schema evolution support and not a complex setup. </p>\\n\\n<p>What are you thoughts on using Snowflake&#39;s Openflow vs debezium vs AWS DMS vs SAAS solution</p>\\n\\n<p>What do you guys use? </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1n4dy5z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='parthsavi'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4dy5z/postgres_to_snowflake_replication_recommendations/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4dy5z/postgres_to_snowflake_replication_recommendations/', 'subreddit_subscribers': 392322, 'created_utc': 1756591523.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.790480Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"I'm having solo hackathon as selection process for DE role and I really want to conquer i have 2 month internship in that company work on data lakehouse and some etl project on ADF and some python and databricks now I am participated in several hackthons but those are based on web and ml and real world problems but not in DE specific hackathon so any good projects or real world problems I can solve and achieve good position in hackthone anyone help me \", 'author_fullname': 't2_f1uc4cv3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': \"I'm having hackathon for data engineer job\", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4zo89', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.65, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756658979.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>I&#39;m having solo hackathon as selection process for DE role and I really want to conquer i have 2 month internship in that company work on data lakehouse and some etl project on ADF and some python and databricks now I am participated in several hackthons but those are based on web and ml and real world problems but not in DE specific hackathon so any good projects or real world problems I can solve and achieve good position in hackthone anyone help me </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1n4zo89', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Alex_0004'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4zo89/im_having_hackathon_for_data_engineer_job/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4zo89/im_having_hackathon_for_data_engineer_job/', 'subreddit_subscribers': 392322, 'created_utc': 1756658979.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.791391Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_1amk2offhs', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The Fastest Way to Insert Data to Postgres', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 41, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4eh1f', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.71, 'author_flair_background_color': None, 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=140&height=41&crop=140:41,smart&auto=webp&s=8f026e9e425f3f94cafa1bebe97344577c61e980', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1756592947.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'confessionsofadataguy.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.confessionsofadataguy.com/the-fastest-way-to-insert-data-to-postgres/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?auto=webp&s=a6b70aa56a5b0516eadee0dc577c7203423df471', 'width': 2000, 'height': 596}, 'resolutions': [{'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=108&crop=smart&auto=webp&s=3e7f16aebc2925d7863fbf2d65e3d4eec9648e71', 'width': 108, 'height': 32}, {'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=216&crop=smart&auto=webp&s=cb746a55ab5c914604c34c2719aa8e13b4f9bc71', 'width': 216, 'height': 64}, {'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=320&crop=smart&auto=webp&s=04b77e716c3626c00d5bcb68c3103cdcb50fa6dc', 'width': 320, 'height': 95}, {'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=640&crop=smart&auto=webp&s=e6f03fb2bccac2fd3249ca3b968392c6c286ddd6', 'width': 640, 'height': 190}, {'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=960&crop=smart&auto=webp&s=a432d059d73ccb944fe36f66e4f16ff2f124e997', 'width': 960, 'height': 286}, {'url': 'https://external-preview.redd.it/ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY.png?width=1080&crop=smart&auto=webp&s=dfd08acd5ae62d97505454bf5cc17aab509a7104', 'width': 1080, 'height': 321}], 'variants': {}, 'id': 'ytEz4feBT9Dkjw-qEegFzMu7JF_SvkDfrAKErX6D-tY'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1n4eh1f', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='averageflatlanders'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4eh1f/the_fastest_way_to_insert_data_to_postgres/', 'stickied': False, 'url': 'https://www.confessionsofadataguy.com/the-fastest-way-to-insert-data-to-postgres/', 'subreddit_subscribers': 392322, 'created_utc': 1756592947.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.792738Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Hey r/dataengineering,\\n\\nLike many of you, I've spent a good chunk of my career being the go-to person for ad-hoc data requests. The constant context-switching to answer simple questions for marketing, sales, or product folks was a huge drain on my productivity.\\n\\nSo, I started working on a side project to see if I could build a better way. The result is something I'm calling DBdash.\\n\\nThe idea is simple: it’s a tool that lets you (or your less-technical stakeholders) ask questions in plain English, and it returns a verified answer, a chart, and just as importantly, the exact SQL query it ran.\\n\\nMy biggest priority was building something that engineers could actually trust. There are no black boxes here. You can audit the SQL for every single query to confirm the logic. The goal isn't to replace analysts or engineers, but to handle that first layer of simple, repetitive questions and free us up for more complex work.\\n\\nIt connects directly to your database (Postgres and MySQL supported for now) and is designed to be set up in a few minutes. Your data stays in your warehouse.\\n\\nI'm getting close to a wider launch and would love to get some honest, direct feedback from the pros in this community.\\n\\n\\\\*   Does this seem like a tool that would actually solve a problem for you?  \\n\\\\*   What are the immediate red flags or potential security concerns that come to mind?  \\n\\\\*   What features would be an absolute must-have for you to consider trying it?\\n\\nYou can check out the landing page here: [https://dbdash.app](https://dbdash.app)\\n\\nIt's still in early access, but I'm really keen to hear what this community thinks. I'm ready for the roast!\\n\\nThanks for your time.\", 'author_fullname': 't2_af02gtey', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'My side project to end the \"can you just pull this data for me?\" requests. Seeking feedback.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4zjka', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756658673.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hey <a href=\"/r/dataengineering\">r/dataengineering</a>,</p>\\n\\n<p>Like many of you, I&#39;ve spent a good chunk of my career being the go-to person for ad-hoc data requests. The constant context-switching to answer simple questions for marketing, sales, or product folks was a huge drain on my productivity.</p>\\n\\n<p>So, I started working on a side project to see if I could build a better way. The result is something I&#39;m calling DBdash.</p>\\n\\n<p>The idea is simple: it’s a tool that lets you (or your less-technical stakeholders) ask questions in plain English, and it returns a verified answer, a chart, and just as importantly, the exact SQL query it ran.</p>\\n\\n<p>My biggest priority was building something that engineers could actually trust. There are no black boxes here. You can audit the SQL for every single query to confirm the logic. The goal isn&#39;t to replace analysts or engineers, but to handle that first layer of simple, repetitive questions and free us up for more complex work.</p>\\n\\n<p>It connects directly to your database (Postgres and MySQL supported for now) and is designed to be set up in a few minutes. Your data stays in your warehouse.</p>\\n\\n<p>I&#39;m getting close to a wider launch and would love to get some honest, direct feedback from the pros in this community.</p>\\n\\n<p>*   Does this seem like a tool that would actually solve a problem for you?<br/>\\n*   What are the immediate red flags or potential security concerns that come to mind?<br/>\\n*   What features would be an absolute must-have for you to consider trying it?</p>\\n\\n<p>You can check out the landing page here: <a href=\"https://dbdash.app\">https://dbdash.app</a></p>\\n\\n<p>It&#39;s still in early access, but I&#39;m really keen to hear what this community thinks. I&#39;m ready for the roast!</p>\\n\\n<p>Thanks for your time.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?auto=webp&s=2138eb21597dd28744f83bfbc1fe48aab3b62446', 'width': 2400, 'height': 1790}, 'resolutions': [{'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?width=108&crop=smart&auto=webp&s=02bc6e542afed3f900f2204b6200838a4eb0284d', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?width=216&crop=smart&auto=webp&s=dd7415b4a59b7ab25adc3d3ceaa0bac6968d2274', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?width=320&crop=smart&auto=webp&s=39b98d3f2b3bb2fa8cc5617198f14a6f746f1c54', 'width': 320, 'height': 238}, {'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?width=640&crop=smart&auto=webp&s=43751a9876762c5057fff2b4b210bf439fe83bfe', 'width': 640, 'height': 477}, {'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?width=960&crop=smart&auto=webp&s=95279e2af8f90e97543129503d6b48b7e9e8f3af', 'width': 960, 'height': 716}, {'url': 'https://external-preview.redd.it/YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA.png?width=1080&crop=smart&auto=webp&s=c1ba36b001d41bf04d69097587ca60d0b5d1403e', 'width': 1080, 'height': 805}], 'variants': {}, 'id': 'YgZi01GGIOxuM3cdTiSeb0mNFNB8jqPu1Be6woKaisA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1n4zjka', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Scary-Ad7000'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4zjka/my_side_project_to_end_the_can_you_just_pull_this/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4zjka/my_side_project_to_end_the_can_you_just_pull_this/', 'subreddit_subscribers': 392322, 'created_utc': 1756658673.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.793854Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"We have a single-table DynamoDB design and are looking for a preferably low-latency sync to a relational datastore for analytics purposes. \\n\\nWe were delighted with Rockset, but they got acquired and shut down. Tinybird has been selling itself as an alternative, and we have been using them, but it doesn't really seem to work that well for this use case. \\n\\nThere is an AWS Kinesis option to S3 or Redshift.   \\n  \\nAre there other 'streaming ETL'  tools like Estuary that could work? What datastore would you use?\\n\\n\\n\\n\", 'author_fullname': 't2_49njupzo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Streaming DynamoDB to a datastore (and we then can run a dashboard on)?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4h2p5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756600413.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>We have a single-table DynamoDB design and are looking for a preferably low-latency sync to a relational datastore for analytics purposes. </p>\\n\\n<p>We were delighted with Rockset, but they got acquired and shut down. Tinybird has been selling itself as an alternative, and we have been using them, but it doesn&#39;t really seem to work that well for this use case. </p>\\n\\n<p>There is an AWS Kinesis option to S3 or Redshift.   </p>\\n\\n<p>Are there other &#39;streaming ETL&#39;  tools like Estuary that could work? What datastore would you use?</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1n4h2p5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='stan-van'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4h2p5/streaming_dynamodb_to_a_datastore_and_we_then_can/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4h2p5/streaming_dynamodb_to_a_datastore_and_we_then_can/', 'subreddit_subscribers': 392322, 'created_utc': 1756600413.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.794970Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"I've been looking for Data Integration Engineer jobs in the healthcare space lately, and that motivated me to build my own, rudimentary data ingestion engine based on how I think tools like Mirth, Rhapsody, or Boomi would work. I wanted to share it here to get feedback, especially from any data engineers working in the healthcare, public health, or healthtech space.\\n\\nThe gist of the project is that it's a Dockerized pipeline that produces synthetic HL7 messages and then passes the data through a series of steps including ingestion, quality assurance checks, and conversion to FHIR. Everything is monitored and tracked with Prometheus and displayed with Grafana. Kafka is used as the message queue, and MinIO is used to replicate an S3 bucket.\\n\\nIf you're the type of person that likes digging around in code, you can check the project out [here](https://github.com/bryanbritten/hl7-integration).\\n\\nIf you're the type of person that would rather watch a video overview, you can check that out [here](https://www.loom.com/share/3de54014865b4cb7aa59b87ec940626d?sid=271f23fd-7ade-4f82-864f-e2a288a96a36).\\n\\nI'd love to get feedback on what I'm getting right and what I could include to better represent my capacity for working as a Data Integration Engineer in healthcare. I am already planning to extend the segments and message types that are generated, and will be adding a terminology server (another Docker service) to facilitate working with LOINC, SNOMED, and IDC-10 values.\\n\\nThanks in advance for checking my project out!\", 'author_fullname': 't2_42gtu2c3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'HL7 Data Integration Pipeline', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4emel', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756593370.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>I&#39;ve been looking for Data Integration Engineer jobs in the healthcare space lately, and that motivated me to build my own, rudimentary data ingestion engine based on how I think tools like Mirth, Rhapsody, or Boomi would work. I wanted to share it here to get feedback, especially from any data engineers working in the healthcare, public health, or healthtech space.</p>\\n\\n<p>The gist of the project is that it&#39;s a Dockerized pipeline that produces synthetic HL7 messages and then passes the data through a series of steps including ingestion, quality assurance checks, and conversion to FHIR. Everything is monitored and tracked with Prometheus and displayed with Grafana. Kafka is used as the message queue, and MinIO is used to replicate an S3 bucket.</p>\\n\\n<p>If you&#39;re the type of person that likes digging around in code, you can check the project out <a href=\"https://github.com/bryanbritten/hl7-integration\">here</a>.</p>\\n\\n<p>If you&#39;re the type of person that would rather watch a video overview, you can check that out <a href=\"https://www.loom.com/share/3de54014865b4cb7aa59b87ec940626d?sid=271f23fd-7ade-4f82-864f-e2a288a96a36\">here</a>.</p>\\n\\n<p>I&#39;d love to get feedback on what I&#39;m getting right and what I could include to better represent my capacity for working as a Data Integration Engineer in healthcare. I am already planning to extend the segments and message types that are generated, and will be adding a terminology server (another Docker service) to facilitate working with LOINC, SNOMED, and IDC-10 values.</p>\\n\\n<p>Thanks in advance for checking my project out!</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?auto=webp&s=564b32908937c74829319259c6dfc7e6e07f9422', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?width=108&crop=smart&auto=webp&s=861f725b0c6fe44fd7431c9fbb156347e4e1f48a', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?width=216&crop=smart&auto=webp&s=812966ee72d550142ab6e7cef5aef153bdf312d7', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?width=320&crop=smart&auto=webp&s=f99c83bb8de448a93e18c56b1f0f4a87f64d4c74', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?width=640&crop=smart&auto=webp&s=e69316c72e4c0429c32666f7ebc49ffc249a7423', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?width=960&crop=smart&auto=webp&s=5815fe1c9ed7ed37235b3a39740071a8b31e927f', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8.png?width=1080&crop=smart&auto=webp&s=8a8698f0232c9246c0cf264e1d35b9fc0671f248', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'tLrVUNi69TUyn6eTAFVZaPuTk-RPMi_HQ_8h2wPPNP8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1n4emel', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='therealtibblesnbits'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1n4emel/hl7_data_integration_pipeline/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4emel/hl7_data_integration_pipeline/', 'subreddit_subscribers': 392322, 'created_utc': 1756593370.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.795871Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Dara Engineering is to comp sci like being a crane operator is to construction.   \\n\\nNo, I can’t help you build a simple app, the same way a crane operator doesn’t innately know how to do finish cabinetry or wire a tool shed. \\n\\nGranted when I shared this comparison with some friends in construction they pointed out that most crane operators are very good jack of all trades. \\n\\nBut I am not. ', 'author_fullname': 't2_ajrnlako', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I figured out how I’m going to describe Data Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1n55ks2', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756673071.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Dara Engineering is to comp sci like being a crane operator is to construction.   </p>\\n\\n<p>No, I can’t help you build a simple app, the same way a crane operator doesn’t innately know how to do finish cabinetry or wire a tool shed. </p>\\n\\n<p>Granted when I shared this comparison with some friends in construction they pointed out that most crane operators are very good jack of all trades. </p>\\n\\n<p>But I am not. </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1n55ks2', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='markwusinich_'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n55ks2/i_figured_out_how_im_going_to_describe_data/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n55ks2/i_figured_out_how_im_going_to_describe_data/', 'subreddit_subscribers': 392322, 'created_utc': 1756673071.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.796880Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Had work experience as a DE in retail, all of the stack is in batch Data engineering. \\nAirflow, DBT, BigQuery, CICD etc and that's pretty much it.\\n\\nI'm hoping to dive into a senior DE or MLE role and I noticed that a lot of the big companies are after Real time streaming experience which I literally never touched before. In terms of background I know a bit of Kubernetes, terraform IAC, kubeflow pipeline as well so more like platform engineering?\\n\\n\\nI have been trying to do a weekend project, for fraud detection, using Kafka, Flink, feast for feature store, fastapi and mlflow. All containerised as microservices using Docker.\\n\\n\\nBut not sure if I'm on the right track though??\\n\\nLink:  https://github.com/lich2000117/streaming-feature-store \\n\\nKeen to hear your thoughts! And I appreciate that 🫡\\n\\n[View Poll](https://www.reddit.com/poll/1n4v1lo)\", 'author_fullname': 't2_16x8rn6j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is streaming knowledge important to march to senior role or MLE?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4v1lo', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756647733.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Had work experience as a DE in retail, all of the stack is in batch Data engineering. \\nAirflow, DBT, BigQuery, CICD etc and that&#39;s pretty much it.</p>\\n\\n<p>I&#39;m hoping to dive into a senior DE or MLE role and I noticed that a lot of the big companies are after Real time streaming experience which I literally never touched before. In terms of background I know a bit of Kubernetes, terraform IAC, kubeflow pipeline as well so more like platform engineering?</p>\\n\\n<p>I have been trying to do a weekend project, for fraud detection, using Kafka, Flink, feast for feature store, fastapi and mlflow. All containerised as microservices using Docker.</p>\\n\\n<p>But not sure if I&#39;m on the right track though??</p>\\n\\n<p>Link:  <a href=\"https://github.com/lich2000117/streaming-feature-store\">https://github.com/lich2000117/streaming-feature-store</a> </p>\\n\\n<p>Keen to hear your thoughts! And I appreciate that 🫡</p>\\n\\n<p><a href=\"https://www.reddit.com/poll/1n4v1lo\">View Poll</a></p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?auto=webp&s=7223677f0dfe13797b4cf4a18a901184635a6639', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?width=108&crop=smart&auto=webp&s=d7e4812e0a4c7de0550bb57a519a77eabde574b7', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?width=216&crop=smart&auto=webp&s=b118422eb9a896340e8c68c60f3bf7e86f54cc3f', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?width=320&crop=smart&auto=webp&s=a5e849fe64e3da801a8f69cf5201a0482d2bc2c9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?width=640&crop=smart&auto=webp&s=c63b554b6bd8b7c195ebae2c00f214547ac315c7', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?width=960&crop=smart&auto=webp&s=0548f397e6fd7b91ea0371af5b044d2fbe0a39a1', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0.png?width=1080&crop=smart&auto=webp&s=4034de88097513daae751507635b97ad9fb9ce40', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'VdM4DbvqVIbrUMpaNXXdKLU9dEku71m15W4ahQifGv0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1n4v1lo', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Lich_Li'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'poll_data': <praw.models.reddit.poll.PollData object at 0x7e4b18dff200>, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4v1lo/is_streaming_knowledge_important_to_march_to/', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4v1lo/is_streaming_knowledge_important_to_march_to/', 'subreddit_subscribers': 392322, 'created_utc': 1756647733.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.797875Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"I hope this is the right sub to get some technical advice. I'm working on replicating the native “Total Sales by Referrer” report inside Shopify using the Fivetran Shopify connector.\\n\\nGoal: match Shopify’s Sales reports 1:1, so stakeholders don’t need to log in to Shopify to see the numbers.\\n\\nWhat I've tried so far:\\n\\n* Built a BigQuery query joining across order, balance\\\\_transaction, and customer\\\\_visit.\\n* Used order.total\\\\_line\\\\_items\\\\_price, total\\\\_discounts, current\\\\_total\\\\_tax, total\\\\_shipping\\\\_price\\\\_set, current\\\\_total\\\\_duties\\\\_set for Shopify’s Gross/Discounts/Tax/Shipping/Duties definitions.\\n* Parsed \\\\*\\\\_set JSON for presentment money vs shop money.\\n* Pulled refunds from balance\\\\_transaction (type='refund') and applied them on the refund date (to match Shopify’s Sales report behavior).\\n* Attribution: pulled utm\\\\_source/utm\\\\_medium/referrer\\\\_url from customer\\\\_visit for last-touch referrer, falling back to order.referring\\\\_site.\\n* Tried to bucket traffic into direct / search / social / referral / email, and recently added a paid-vs-organic distinction (using UTM mediums and click IDs like gclid/fbclid).\\n* For shipping country, we discovered Fivetran Shopify schema doesn’t always expose it consistently (sometimes as shipping\\\\_address\\\\_country, sometimes shipping\\\\_country), so we started parsing from the JSON row as a fallback.\\n\\nBut nothing seems to match up, and I can't find the fields I need directly either. This is my first time trying to do something like this so I'm honestly lost on what I should be doing.\\n\\nIf you’ve solved this problem before, I’d love to hear:\\n\\n* Which tables/fields you leaned on\\n* How you handle attribution and refunds\\n* Any pitfalls you ran into with Fivetran’s schema\\n* Or even SQL snippets I could copy\\n\\nNote: This is a small time project I'm not looking to hire anyone to do\", 'author_fullname': 't2_15n4iodjka', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Replicating ShopifyQL “Total Sales by Referrer” in BigQuery (with Fivetran Shopify schema)?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4jy50', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756609239.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>I hope this is the right sub to get some technical advice. I&#39;m working on replicating the native “Total Sales by Referrer” report inside Shopify using the Fivetran Shopify connector.</p>\\n\\n<p>Goal: match Shopify’s Sales reports 1:1, so stakeholders don’t need to log in to Shopify to see the numbers.</p>\\n\\n<p>What I&#39;ve tried so far:</p>\\n\\n<ul>\\n<li>Built a BigQuery query joining across order, balance_transaction, and customer_visit.</li>\\n<li>Used order.total_line_items_price, total_discounts, current_total_tax, total_shipping_price_set, current_total_duties_set for Shopify’s Gross/Discounts/Tax/Shipping/Duties definitions.</li>\\n<li>Parsed *_set JSON for presentment money vs shop money.</li>\\n<li>Pulled refunds from balance_transaction (type=&#39;refund&#39;) and applied them on the refund date (to match Shopify’s Sales report behavior).</li>\\n<li>Attribution: pulled utm_source/utm_medium/referrer_url from customer_visit for last-touch referrer, falling back to order.referring_site.</li>\\n<li>Tried to bucket traffic into direct / search / social / referral / email, and recently added a paid-vs-organic distinction (using UTM mediums and click IDs like gclid/fbclid).</li>\\n<li>For shipping country, we discovered Fivetran Shopify schema doesn’t always expose it consistently (sometimes as shipping_address_country, sometimes shipping_country), so we started parsing from the JSON row as a fallback.</li>\\n</ul>\\n\\n<p>But nothing seems to match up, and I can&#39;t find the fields I need directly either. This is my first time trying to do something like this so I&#39;m honestly lost on what I should be doing.</p>\\n\\n<p>If you’ve solved this problem before, I’d love to hear:</p>\\n\\n<ul>\\n<li>Which tables/fields you leaned on</li>\\n<li>How you handle attribution and refunds</li>\\n<li>Any pitfalls you ran into with Fivetran’s schema</li>\\n<li>Or even SQL snippets I could copy</li>\\n</ul>\\n\\n<p>Note: This is a small time project I&#39;m not looking to hire anyone to do</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1n4jy50', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='EveningUnlikely7253'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4jy50/replicating_shopifyql_total_sales_by_referrer_in/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4jy50/replicating_shopifyql_total_sales_by_referrer_in/', 'subreddit_subscribers': 392322, 'created_utc': 1756609239.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.799155Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '[https://nomas.fyi](https://nomas.fyi) \\n\\nIn case you guys wondering,  I have my own AWS RDS and EC2 so I have total control of the data,  I cleaned the SEC filings (3,4,5, 13F, company fundamentals).\\n\\nLet me know what do you guys think. I know there are a lot of products out there. But they either have API only or Visualization only or very expensive.\\n\\n', 'author_fullname': 't2_1kunweljpk', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'is_gallery': True, 'title': 'I just open up the compiled SEC data API + API key for easy test/migration/AI feed', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 72, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ohhds9jlm9mf1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 56, 'x': 108, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=108&crop=smart&auto=webp&s=72563767a96fda9017c0665951ba9f12c1d37be2'}, {'y': 112, 'x': 216, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=216&crop=smart&auto=webp&s=3f2134033f092bdeeb92f38e40ae827839f01c0e'}, {'y': 166, 'x': 320, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=320&crop=smart&auto=webp&s=5e0d2035a247ef36d295e0294ad064345d712a44'}, {'y': 332, 'x': 640, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=640&crop=smart&auto=webp&s=2e9f6c8659a8edf0302c41fc1f94f67376d7abd6'}, {'y': 498, 'x': 960, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=960&crop=smart&auto=webp&s=ae32cf674bf75d633e99fba11eaf150260482f63'}, {'y': 561, 'x': 1080, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=1080&crop=smart&auto=webp&s=b07e94cd93c74b5301533a080af205beded68caf'}], 's': {'y': 1744, 'x': 3356, 'u': 'https://preview.redd.it/ohhds9jlm9mf1.png?width=3356&format=png&auto=webp&s=907b285eec95153f78335c3109d727a52be08125'}, 'id': 'ohhds9jlm9mf1'}, 'cerzilymm9mf1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 56, 'x': 108, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=108&crop=smart&auto=webp&s=57b164cc3d91ef8f09135fe4fc6aebba5a0e8566'}, {'y': 112, 'x': 216, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=216&crop=smart&auto=webp&s=ea3fc4242fdc97973d91bca1e445ed0af07030b9'}, {'y': 166, 'x': 320, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=320&crop=smart&auto=webp&s=3338d2313cebf9a962a70e04bdbc8f85d1eaa1bc'}, {'y': 332, 'x': 640, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=640&crop=smart&auto=webp&s=017036c93c823a8a23be59ec4dac7abbbc7595f1'}, {'y': 498, 'x': 960, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=960&crop=smart&auto=webp&s=c7605b3d40937e4ab5625b4f63c34d9ba622f9b8'}, {'y': 561, 'x': 1080, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=1080&crop=smart&auto=webp&s=d61dbbf1d5114a2685c7a5793fb9152ee8c2b66b'}], 's': {'y': 1744, 'x': 3356, 'u': 'https://preview.redd.it/cerzilymm9mf1.png?width=3356&format=png&auto=webp&s=ad9a8a7cf4b2a67643e29094f9990f86c68b58cb'}, 'id': 'cerzilymm9mf1'}}, 'name': 't3_1n4jc94', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'ups': 2, 'domain': 'reddit.com', 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'gallery_data': {'items': [{'media_id': 'ohhds9jlm9mf1', 'id': 740128022}, {'media_id': 'cerzilymm9mf1', 'id': 740128023}]}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/4CmkOkgxxNMkwAmwCCXLuY1tlrvZQ2paz34UJmZhg54.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1756607314.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'total_awards_received': 0, 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://nomas.fyi\">https://nomas.fyi</a> </p>\\n\\n<p>In case you guys wondering,  I have my own AWS RDS and EC2 so I have total control of the data,  I cleaned the SEC filings (3,4,5, 13F, company fundamentals).</p>\\n\\n<p>Let me know what do you guys think. I know there are a lot of products out there. But they either have API only or Visualization only or very expensive.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.reddit.com/gallery/1n4jc94', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1n4jc94', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ccnomas'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4jc94/i_just_open_up_the_compiled_sec_data_api_api_key/', 'stickied': False, 'url': 'https://www.reddit.com/gallery/1n4jc94', 'subreddit_subscribers': 392322, 'created_utc': 1756607314.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.800107Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Hi everyone,\\nI’m running into an odd issue with Fabric pipelines / ADF integration and hoping someone has seen this before.\\n\\nI have a stored procedure in Fabric Warehouse that uses OPENROWSET(BULK …, FORMAT='PARQUET') to load data from OneLake (ADLS mounted).\\n\\nWhen I execute the proc manually in the Fabric workspace using my personal account, it works fine and the parquet data loads into the table.\\n\\nHowever, when I try to run the same proc through:\\n\\nan ADF pipeline (linked service with a service principal), or\\n\\na Fabric pipeline that invokes the proc with the same service principal,\\nthe proc runs but fails to actually read from OneLake. The table is created but no data is inserted.\\n\\n\\n\\nBoth my personal account and the SPN have the same OneLake read access assigned.\\n\\nSo far it looks like a permissions / tenant setting issue, but I’m not sure which toggle or role is missing for the service principal.\\n\\nHas anyone run into this mismatch where OPENROWSET works interactively but not via service principals in pipelines? Any guidance on the required Fabric tenant settings or item-level permissions would be hugely appreciated.\\n\\nThanks!\", 'author_fullname': 't2_c61b1xyj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Service principal can’t read OneLake files via OPENROWSET in Fabric Warehouse, but works with personal account', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n526b5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756664924.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,\\nI’m running into an odd issue with Fabric pipelines / ADF integration and hoping someone has seen this before.</p>\\n\\n<p>I have a stored procedure in Fabric Warehouse that uses OPENROWSET(BULK …, FORMAT=&#39;PARQUET&#39;) to load data from OneLake (ADLS mounted).</p>\\n\\n<p>When I execute the proc manually in the Fabric workspace using my personal account, it works fine and the parquet data loads into the table.</p>\\n\\n<p>However, when I try to run the same proc through:</p>\\n\\n<p>an ADF pipeline (linked service with a service principal), or</p>\\n\\n<p>a Fabric pipeline that invokes the proc with the same service principal,\\nthe proc runs but fails to actually read from OneLake. The table is created but no data is inserted.</p>\\n\\n<p>Both my personal account and the SPN have the same OneLake read access assigned.</p>\\n\\n<p>So far it looks like a permissions / tenant setting issue, but I’m not sure which toggle or role is missing for the service principal.</p>\\n\\n<p>Has anyone run into this mismatch where OPENROWSET works interactively but not via service principals in pipelines? Any guidance on the required Fabric tenant settings or item-level permissions would be hugely appreciated.</p>\\n\\n<p>Thanks!</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1n526b5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pragi_03'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n526b5/service_principal_cant_read_onelake_files_via/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n526b5/service_principal_cant_read_onelake_files_via/', 'subreddit_subscribers': 392322, 'created_utc': 1756664924.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.801060Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I need sufficient data to train and test a machine learning model which predicts if the health of the patient will deteriorate within the next 90 days based on patient data from the past 30-180 days. ', 'author_fullname': 't2_rcm9zx4jk', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I have a limited set of patient ICU data(vitals, labs, medication etc). How do I create more synthetic data based on the data I have?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n528w5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.43, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756665093.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>I need sufficient data to train and test a machine learning model which predicts if the health of the patient will deteriorate within the next 90 days based on patient data from the past 30-180 days. </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1n528w5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok-Blacksmith3087'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n528w5/i_have_a_limited_set_of_patient_icu_datavitals/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n528w5/i_have_a_limited_set_of_patient_icu_datavitals/', 'subreddit_subscribers': 392322, 'created_utc': 1756665093.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
{"timestamp":"2025-08-31T21:41:41.802066Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7e4b1df30440>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '## **Internship Offer**\\n\\n**Role:** Python ETL / Data Pipeline Engineering Intern – Real-Time QuestDB Pipeline\\n**Location:** Remote (India)\\n\\n---\\n\\n### **About the Project**\\n\\nWe are building a **real-time ETL pipeline** for processing Claude Code conversation logs:\\n\\n* **Extracts** real-time log data\\n* **Transforms** it into structured events (timestamps, session metadata, tagging)\\n* **Loads** it into **QuestDB** for analytics and monitoring\\n\\nThe system works but needs **debugging** and **enterprise-level upgrades** to meet production standards. This internship offers hands-on experience with **real-time data engineering** and **Python ETL pipelines** in a practical, open-source setting.\\n\\n---\\n\\n### **Open Source Project**\\n\\nInterns will work on the [AI-Agent-Host](https://github.com/quantiota/AI-Agent-Host) repository.\\n\\n* Install the AI Agent Host with the provided scripts and **Claude Code** under your own subscription.\\n* Contribute to **bug fixes, performance improvements, and pipeline enhancements**.\\n* Submit progress updates and propose improvements.\\n\\n---\\n\\n### **Internship Details**\\n\\n* **Duration:** 3 Months\\n* **Location:** Remote (India)\\n* **Stipend:** 10,000 INR / month\\n* **Lunch Allowance:** 4,000 INR / month\\n* **Start Date:** Flexible within the next month\\n\\n---\\n\\n### **Responsibilities**\\n\\n* Debug existing ETL scripts (log tailing, parsing, QuestDB inserts)\\n* Implement reliable **Extract → Transform → Load** workflows with error handling and retries\\n* Add **unit tests**, **structured logging**, and **basic monitoring**\\n* Explore **QuestDB ILP ingestion** for high-throughput writes\\n* Deliver **documentation** for setup, usage, and pipeline upgrades\\n\\n---\\n\\n### **Required Skills**\\n\\n* **Python 3** programming\\n* Basic understanding of **data pipelines** and **ETL workflows**\\n* Knowledge of **time-series databases** (QuestDB preferred)\\n* Familiarity with **Docker** and **shell scripting** is a plus\\n\\n---\\n\\n### **Benefits**\\n\\n* Work **remotely** from anywhere in India\\n* Hands-on experience with **real-time streaming systems**\\n* Contribution to an **open-source project** with real-world impact\\n* Mentorship in **enterprise-grade data engineering practices**\\n* Internship certificate upon successful completion\\n\\n---\\n\\n### **How to Apply**\\n\\nPlease share:\\n\\n1. A brief introduction and any relevant coursework/projects\\n2. GitHub or portfolio links (if available)\\n3. Your availability for the 3-month internship period\\n', 'author_fullname': 't2_gju1pg8j1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Python ETL / Data Pipeline Engineering Intern – Real-Time QuestDB Pipeline - Remote (India)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1n4ply7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1756629577.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><h2><strong>Internship Offer</strong></h2>\\n\\n<p><strong>Role:</strong> Python ETL / Data Pipeline Engineering Intern – Real-Time QuestDB Pipeline\\n<strong>Location:</strong> Remote (India)</p>\\n\\n<hr/>\\n\\n<h3><strong>About the Project</strong></h3>\\n\\n<p>We are building a <strong>real-time ETL pipeline</strong> for processing Claude Code conversation logs:</p>\\n\\n<ul>\\n<li><strong>Extracts</strong> real-time log data</li>\\n<li><strong>Transforms</strong> it into structured events (timestamps, session metadata, tagging)</li>\\n<li><strong>Loads</strong> it into <strong>QuestDB</strong> for analytics and monitoring</li>\\n</ul>\\n\\n<p>The system works but needs <strong>debugging</strong> and <strong>enterprise-level upgrades</strong> to meet production standards. This internship offers hands-on experience with <strong>real-time data engineering</strong> and <strong>Python ETL pipelines</strong> in a practical, open-source setting.</p>\\n\\n<hr/>\\n\\n<h3><strong>Open Source Project</strong></h3>\\n\\n<p>Interns will work on the <a href=\"https://github.com/quantiota/AI-Agent-Host\">AI-Agent-Host</a> repository.</p>\\n\\n<ul>\\n<li>Install the AI Agent Host with the provided scripts and <strong>Claude Code</strong> under your own subscription.</li>\\n<li>Contribute to <strong>bug fixes, performance improvements, and pipeline enhancements</strong>.</li>\\n<li>Submit progress updates and propose improvements.</li>\\n</ul>\\n\\n<hr/>\\n\\n<h3><strong>Internship Details</strong></h3>\\n\\n<ul>\\n<li><strong>Duration:</strong> 3 Months</li>\\n<li><strong>Location:</strong> Remote (India)</li>\\n<li><strong>Stipend:</strong> 10,000 INR / month</li>\\n<li><strong>Lunch Allowance:</strong> 4,000 INR / month</li>\\n<li><strong>Start Date:</strong> Flexible within the next month</li>\\n</ul>\\n\\n<hr/>\\n\\n<h3><strong>Responsibilities</strong></h3>\\n\\n<ul>\\n<li>Debug existing ETL scripts (log tailing, parsing, QuestDB inserts)</li>\\n<li>Implement reliable <strong>Extract → Transform → Load</strong> workflows with error handling and retries</li>\\n<li>Add <strong>unit tests</strong>, <strong>structured logging</strong>, and <strong>basic monitoring</strong></li>\\n<li>Explore <strong>QuestDB ILP ingestion</strong> for high-throughput writes</li>\\n<li>Deliver <strong>documentation</strong> for setup, usage, and pipeline upgrades</li>\\n</ul>\\n\\n<hr/>\\n\\n<h3><strong>Required Skills</strong></h3>\\n\\n<ul>\\n<li><strong>Python 3</strong> programming</li>\\n<li>Basic understanding of <strong>data pipelines</strong> and <strong>ETL workflows</strong></li>\\n<li>Knowledge of <strong>time-series databases</strong> (QuestDB preferred)</li>\\n<li>Familiarity with <strong>Docker</strong> and <strong>shell scripting</strong> is a plus</li>\\n</ul>\\n\\n<hr/>\\n\\n<h3><strong>Benefits</strong></h3>\\n\\n<ul>\\n<li>Work <strong>remotely</strong> from anywhere in India</li>\\n<li>Hands-on experience with <strong>real-time streaming systems</strong></li>\\n<li>Contribution to an <strong>open-source project</strong> with real-world impact</li>\\n<li>Mentorship in <strong>enterprise-grade data engineering practices</strong></li>\\n<li>Internship certificate upon successful completion</li>\\n</ul>\\n\\n<hr/>\\n\\n<h3><strong>How to Apply</strong></h3>\\n\\n<p>Please share:</p>\\n\\n<ol>\\n<li>A brief introduction and any relevant coursework/projects</li>\\n<li>GitHub or portfolio links (if available)</li>\\n<li>Your availability for the 3-month internship period</li>\\n</ol>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?auto=webp&s=4ec78f4406a610d0b763ed745ac24f9137823d79', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?width=108&crop=smart&auto=webp&s=c04651f12080fdfffd67e2cd49b480dec44241b7', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?width=216&crop=smart&auto=webp&s=42c64373fde1d50b7e0fd114da64f6a4c187d51b', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?width=320&crop=smart&auto=webp&s=91b23630a87f110f535f2a60da1c79ba311fd1fe', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?width=640&crop=smart&auto=webp&s=5738a6cdc34a3ca49b3f990fd4e82e33a9acbab7', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?width=960&crop=smart&auto=webp&s=7baa7ffb083d96dd20e74c174bd3d8e4bff45877', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI.png?width=1080&crop=smart&auto=webp&s=73480cb6cfd35f60d2237ea13f2bdecc9d1e17b4', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'pvds4qJdRGPVjM14vCLj6ZIGKRd-F5nHVJUAsrFKXsI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1n4ply7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Emotional-Access-227'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1n4ply7/python_etl_data_pipeline_engineering_intern/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1n4ply7/python_etl_data_pipeline_engineering_intern/', 'subreddit_subscribers': 392322, 'created_utc': 1756629577.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","chan":"stdout","logger":"task"}
